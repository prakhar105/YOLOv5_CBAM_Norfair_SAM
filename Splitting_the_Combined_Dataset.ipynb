{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gufNZR5TPLM-"
      },
      "source": [
        "# **Splitting the Datasets**\n",
        "\n",
        "Here, the datasets which were cleaned and augmented in the Dataset_Combination file and then manually combined, are further split into training, test, and validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h3f8yUBPWd8"
      },
      "source": [
        "# **Step 1: Install required dependencies:**\n",
        "\n",
        "In this step, I install essential libraries that will be used for augmentation, image processing, and file handling. The following packages are installed:\n",
        "\n",
        "*   albumentations, for image augmentation\n",
        "*   opencv-python, for image processing tasks\n",
        "*   pillow, for handling image file formats\n",
        "*   numpy, for numerical operations\n",
        "*   gdown, for downloading files from GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IJSqa-C_4Tks",
        "outputId": "1817576e-2348-4e10-8ffe-d7fd7d272fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting albumentations==1.0.3\n",
            "  Downloading albumentations-1.0.3-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting opencv-python==4.5.3.56\n",
            "  Downloading opencv-python-4.5.3.56.tar.gz (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.1.0\n",
            "    Uninstalling gdown-5.1.0:\n",
            "      Successfully uninstalled gdown-5.1.0\n",
            "Successfully installed gdown-5.2.0\n"
          ]
        }
      ],
      "source": [
        "#Set up libraries and dependencies\n",
        "!pip install albumentations==1.0.3 opencv-python==4.5.3.56 pillow numpy\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8ix7MsAPytJ"
      },
      "source": [
        "# **Step 2: Set Up the Environment**\n",
        "\n",
        "This section mounts the GDrive to the Colab environment to access files and directories stored ther. The dataset and other related files will be located and stored in GDrive until manual download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PTDFxTfNSdYB",
        "outputId": "626465a9-f137-46d2-e539-1a54fabdbfca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Set up environment\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5da4q0gQDqB"
      },
      "source": [
        "#**Step 3: Augmentation (Obsolete)**\n",
        "\n",
        "The following code was originally used to apply further augmentation steps to images in the dataset, however I decided it was best to do this in the original handling of the separate datasets, to prevent increased likeliness of corrupt files.\n",
        "\n",
        "The code below has been left as reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vFQlbY0a4ewu"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mA\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "#Augmentation: obsolete\n",
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths to dataset\n",
        "dataset_path = '/content/drive/MyDrive/create_data/complete_dataset'\n",
        "images_path = os.path.join(dataset_path, 'images')\n",
        "labels_path = os.path.join(dataset_path, 'labels')\n",
        "\n",
        "# Output paths\n",
        "augmented_images_path = os.path.join(dataset_path, 'augmented_images')\n",
        "augmented_labels_path = os.path.join(dataset_path, 'augmented_labels')\n",
        "\n",
        "# Create directories if they do not exist\n",
        "os.makedirs(augmented_images_path, exist_ok=True)\n",
        "os.makedirs(augmented_labels_path, exist_ok=True)\n",
        "\n",
        "# Define augmentation pipeline\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.5),\n",
        "    A.GaussianBlur(p=0.2),\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "# Function to read YOLO labels\n",
        "def read_yolo_labels(label_path):\n",
        "    with open(label_path, 'r') as file:\n",
        "        labels = []\n",
        "        for line in file:\n",
        "            class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "            labels.append([x_center, y_center, width, height, class_id])\n",
        "    return labels\n",
        "\n",
        "# Function to write YOLO labels\n",
        "def write_yolo_labels(label_path, labels):\n",
        "    with open(label_path, 'w') as file:\n",
        "        for label in labels:\n",
        "            x_center, y_center, width, height, class_id = label\n",
        "            file.write(f\"{int(class_id)} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "# Augment images and labels\n",
        "for image_filename in tqdm(os.listdir(images_path)):\n",
        "    if image_filename.endswith('.jpg'):\n",
        "        image_path = os.path.join(images_path, image_filename)\n",
        "        label_path = os.path.join(labels_path, image_filename.replace('.jpg', '.txt'))\n",
        "\n",
        "        # Read image and labels\n",
        "        image = np.array(Image.open(image_path).convert('RGB'))\n",
        "        bboxes = read_yolo_labels(label_path)\n",
        "        class_labels = [label[4] for label in bboxes]\n",
        "\n",
        "        # Apply augmentation\n",
        "        augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "        augmented_image = augmented['image']\n",
        "        augmented_bboxes = augmented['bboxes']\n",
        "\n",
        "        # Save augmented image and labels\n",
        "        augmented_image_path = os.path.join(augmented_images_path, image_filename)\n",
        "        augmented_label_path = os.path.join(augmented_labels_path, image_filename.replace('.jpg', '.txt'))\n",
        "\n",
        "        Image.fromarray(augmented_image).save(augmented_image_path)\n",
        "        write_yolo_labels(augmented_label_path, augmented_bboxes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVHZdQmMQkSH"
      },
      "source": [
        "#**Step 4: Split Dataset into Train, Test, and Validation Sets**\n",
        "\n",
        "In this step, the unified, cleaned, and augmented datasets which have been unified are split into training, test, and validation sets for use with the custom YOLOv5 model for the current project.\n",
        "\n",
        "The dataset is split into 80% for training, 20% for testing, and, from the training set, 25% is used for validation as well. This is done using the train_test_split function from the sklearn library, ensuring a fair random divide between the data, while maintaining correct proportions.\n",
        "\n",
        "The paths are defined, copied into their respective directories, and then later zipped in the main project notebook.\n",
        "\n",
        "Note: These steps are performed in separate notebooks to reduce the clutter within the main project notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_PDaXDvSaSP"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/final_unified_dataset')\n",
        "\n",
        "# Paths to the augmented dataset\n",
        "dataset_path = '/content/drive/MyDrive/final_unified_dataset'\n",
        "images_path = os.path.join(dataset_path, 'images')\n",
        "labels_path = os.path.join(dataset_path, 'labels')\n",
        "\n",
        "# Output paths for split datasets\n",
        "output_base_path = '/content/drive/MyDrive/split_dataset'\n",
        "train_images_path = os.path.join(output_base_path, 'train', 'images')\n",
        "train_labels_path = os.path.join(output_base_path, 'train', 'labels')\n",
        "val_images_path = os.path.join(output_base_path, 'val', 'images')\n",
        "val_labels_path = os.path.join(output_base_path, 'val', 'labels')\n",
        "test_images_path = os.path.join(output_base_path, 'test', 'images')\n",
        "test_labels_path = os.path.join(output_base_path, 'test', 'labels')\n",
        "\n",
        "# Create directories if they do not exist\n",
        "os.makedirs(train_images_path, exist_ok=True)\n",
        "os.makedirs(train_labels_path, exist_ok=True)\n",
        "os.makedirs(val_images_path, exist_ok=True)\n",
        "os.makedirs(val_labels_path, exist_ok=True)\n",
        "os.makedirs(test_images_path, exist_ok=True)\n",
        "os.makedirs(test_labels_path, exist_ok=True)\n",
        "\n",
        "# List all image files\n",
        "image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
        "\n",
        "# Split the dataset\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Function to copy files to destination\n",
        "def copy_files(files, src_img_dir, src_lbl_dir, dst_img_dir, dst_lbl_dir):\n",
        "    for file in files:\n",
        "        shutil.copy(os.path.join(src_img_dir, file), os.path.join(dst_img_dir, file))\n",
        "        label_file = file.replace('.jpg', '.txt')\n",
        "        shutil.copy(os.path.join(src_lbl_dir, label_file), os.path.join(dst_lbl_dir, label_file))\n",
        "\n",
        "# Copy the files to the respective directories\n",
        "copy_files(train_files, images_path, labels_path, train_images_path, train_labels_path)\n",
        "copy_files(val_files, images_path, labels_path, val_images_path, val_labels_path)\n",
        "copy_files(test_files, images_path, labels_path, test_images_path, test_labels_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
