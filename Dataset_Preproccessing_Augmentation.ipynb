{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Cleaning, Augmentation, and Combination for the Chosen Roboflow Datasets**\n",
        "\n",
        "## First Section: Environment Setup\n",
        "\n",
        "In the below two modules of code, the GDrive is mounted and the dataset file in my GDrive is set to the current directory."
      ],
      "metadata": {
        "id": "pLR16JDZ8Eel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NOTE: Class 0 = headset, 1 = Mouse, 2 = keyboard, 3 = PC\n",
        "#Step 1: Loading the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "p4BLMbcmsI3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOF-9y84q_cN"
      },
      "outputs": [],
      "source": [
        "#Step 2: Setting the file in my GDrive that has the datasets to current directory\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/create_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset # 6**\n",
        "\n",
        "## Updating label files, deleting corrupt label files, and deleting corresponding images without label files.\n",
        "\n",
        "The initial paths point to the label directories for train, val, and test datasets for Dataset 6. These label files are processed to standardise class labels.\n",
        "\n",
        "The update labels function processes each label file in the dataset directories, reads the labels, and updates them based on the predefined mappings:\n",
        "\n",
        "Class 4 is changed to 0.\n",
        "Class 6 is changed to 1.\n",
        "Class 0 is changed to 2.\n",
        "Classes 2, 3, 5, 7, 8 are discarded (ignored).\n",
        "\n",
        "After updating these labels, in the immediately following block of code, image files without valid labels are deleted, ensuring that only correctly labeled images are included in the dataset.\n",
        "\n",
        "The update_labels() function is applied to the tain, val, and test datasets to ensure all three are updated and consistent."
      ],
      "metadata": {
        "id": "QWEa2L-st1fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset 6 label update, deletion, and corresponding image management.\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Path to the label files\n",
        "train_labels_path = 'data6/train/labels'\n",
        "val_labels_path = 'data6/valid/labels'\n",
        "test_labels_path = 'data6/test/labels'\n",
        "\n",
        "# Function to update labels in a given path\n",
        "def update_labels(path):\n",
        "    for label_file in os.listdir(path):\n",
        "        if label_file.endswith('.txt'):\n",
        "            file_path = os.path.join(path, label_file)\n",
        "            updated_lines = []\n",
        "            with open(file_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    label, *coords = line.split()\n",
        "                    label = int(label)\n",
        "                    if label == 4:\n",
        "                        label = 0\n",
        "                    elif label == 6:\n",
        "                        label = 1\n",
        "                    elif label == 0:\n",
        "                        label = 2\n",
        "                    elif label in {2, 3, 5, 7, 8}:\n",
        "                        continue\n",
        "                    updated_lines.append(f\"{label} {' '.join(coords)}\\n\")\n",
        "\n",
        "            if updated_lines:\n",
        "                with open(file_path, 'w') as file:\n",
        "                    file.writelines(updated_lines)\n",
        "            else:\n",
        "                os.remove(file_path)\n",
        "\n",
        "# Update labels in train, validation, and test sets\n",
        "for path in [train_labels_path, val_labels_path, test_labels_path]:\n",
        "    update_labels(path)\n",
        "print(\"Labels updated successfully.\")\n",
        "########################################################"
      ],
      "metadata": {
        "id": "xQF2HwD0tFkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to image and label directories\n",
        "train_images_path = 'data4/train/images'\n",
        "val_images_path = 'data4/valid/images'\n",
        "test_images_path = 'data4/test/images'\n",
        "\n",
        "train_labels_path = 'data4/train/labels'\n",
        "val_labels_path = 'data4/valid/labels'\n",
        "test_labels_path = 'data4/test/labels'\n",
        "\n",
        "# Function to delete images without corresponding label files\n",
        "def delete_images_without_labels(images_path, labels_path):\n",
        "    for image_file in os.listdir(images_path):\n",
        "        if image_file.endswith(('.jpg', '.jpeg', '.png', '.webp')):\n",
        "            label_file = os.path.splitext(image_file)[0] + '.txt'\n",
        "            label_file_path = os.path.join(labels_path, label_file)\n",
        "            if not os.path.exists(label_file_path):\n",
        "                os.remove(os.path.join(images_path, image_file))\n",
        "                print(f\"Deleted {image_file} as it has no corresponding label file.\")\n",
        "\n",
        "# Delete images without labels in train, validation, and test sets\n",
        "for images_path, labels_path in [(train_images_path, train_labels_path),\n",
        "                                 (val_images_path, val_labels_path),\n",
        "                                 (test_images_path, test_labels_path)]:\n",
        "    delete_images_without_labels(images_path, labels_path)\n",
        "\n",
        "print(\"Completed deleting images without corresponding label files.\")\n"
      ],
      "metadata": {
        "id": "InGRGGmTt7wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset # 05**\n",
        "\n",
        "## Updating and standardising label files.\n",
        "\n",
        "This step follows the same structure as with dataset 6, however this dataset did not need as much management of corrupt label files. As such, images without label files did not occur and did not need to be deleted.\n",
        "\n",
        "The update labels function reads and updates the lbales in specified directories, remapping class 0 to class 1. It then updates the file with the new labels."
      ],
      "metadata": {
        "id": "kWSbWQPhuUwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Paths to the label directories\n",
        "train_labels_path = 'data5/train/labels'\n",
        "val_labels_path = 'data5/valid/labels'\n",
        "test_labels_path = 'data5/test/labels'\n",
        "\n",
        "# Function to update labels in a given path\n",
        "def update_labels(path):\n",
        "    for label_file in os.listdir(path):\n",
        "        if label_file.endswith('.txt'):\n",
        "            file_path = os.path.join(path, label_file)\n",
        "            updated_lines = []\n",
        "            with open(file_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    label, *coords = line.split()\n",
        "                    label = int(label)\n",
        "                    if label == 0:\n",
        "                        label = 1\n",
        "                    updated_lines.append(f\"{label} {' '.join(coords)}\\n\")\n",
        "\n",
        "            with open(file_path, 'w') as file:\n",
        "                file.writelines(updated_lines)\n",
        "\n",
        "# Update labels in train, validation, and test sets\n",
        "for path in [train_labels_path, val_labels_path, test_labels_path]:\n",
        "    update_labels(path)\n",
        "\n",
        "print(\"Labels updated successfully.\")\n",
        "##################################################################"
      ],
      "metadata": {
        "id": "CO96e_JRt73S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset # 04**\n",
        "\n",
        "## Updating and standardising label files, deleting image files without labels.\n",
        "\n",
        "This step follows the same structure as with dataset 6, however this dataset did not need as much management of corrupt label files.\n",
        "\n",
        "The update labels function reads and updates the lbales in specified directories, remapping class 0 to class 3, and class 1 is ignored (removed). It then updates the file with the new labels. Files without labels are also deleted."
      ],
      "metadata": {
        "id": "Qc3HLq-FufrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the label directories\n",
        "train_labels_path = 'data4/train/labels'\n",
        "val_labels_path = 'data4/valid/labels'\n",
        "test_labels_path = 'data4/test/labels'\n",
        "\n",
        "# Function to update labels in a given path\n",
        "def update_labels(path):\n",
        "    for label_file in os.listdir(path):\n",
        "        if label_file.endswith('.txt'):\n",
        "            file_path = os.path.join(path, label_file)\n",
        "            updated_lines = []\n",
        "            with open(file_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    label, *coords = line.split()\n",
        "                    label = int(label)\n",
        "                    if label == 0:\n",
        "                        label = 3\n",
        "                    elif label == 1:\n",
        "                        continue\n",
        "                    updated_lines.append(f\"{label} {' '.join(coords)}\\n\")\n",
        "\n",
        "            if updated_lines:\n",
        "                with open(file_path, 'w') as file:\n",
        "                    file.writelines(updated_lines)\n",
        "            else:\n",
        "                os.remove(file_path)\n",
        "\n",
        "# Update labels in train, validation, and test sets\n",
        "for path in [train_labels_path, val_labels_path, test_labels_path]:\n",
        "    update_labels(path)\n",
        "print(\"Labels updated successfully.\")\n",
        "######################################################################"
      ],
      "metadata": {
        "id": "Z-m8nqkWuXmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset # 03**\n",
        "\n",
        "## Updating and standardising label files, deleting image files without labels.\n",
        "\n",
        "This step follows the same structure as with dataset 6, however this dataset did not need as much management of corrupt label files.\n",
        "\n",
        "Class 0 is kept unchanged in this dataset, the update_labels() function simply iterates over the label files, ensuring that all files contain valid labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZbCczxFus8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Paths to the label directories\n",
        "train_labels_path = 'data3/train/labels'\n",
        "val_labels_path = 'data3/valid/labels'\n",
        "test_labels_path = 'data3/test/labels'\n",
        "\n",
        "# Function to update labels in a given path\n",
        "def update_labels(path):\n",
        "    for label_file in os.listdir(path):\n",
        "        if label_file.endswith('.txt'):\n",
        "            file_path = os.path.join(path, label_file)\n",
        "            updated_lines = []\n",
        "            with open(file_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    label, *coords = line.split()\n",
        "                    label = int(label)\n",
        "                    if label == 0:\n",
        "                        label = 0\n",
        "                    updated_lines.append(f\"{label} {' '.join(coords)}\\n\")\n",
        "\n",
        "            with open(file_path, 'w') as file:\n",
        "                file.writelines(updated_lines)\n",
        "\n",
        "# Update labels in train, validation, and test sets\n",
        "for path in [train_labels_path]:\n",
        "    update_labels(path)\n",
        "\n",
        "print(\"Labels updated successfully.\")\n"
      ],
      "metadata": {
        "id": "v7V82X2yuX0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset # 02**\n",
        "\n",
        "## Updating and standardising label files, deleting image files without labels.\n",
        "\n",
        "This step follows the same structure as with dataset 6, however this dataset did not need as much management of corrupt label files.\n",
        "\n",
        "The update labels function reads and updates the lbales in specified directories, remapping class labels 0, 1, and 2, to class 3."
      ],
      "metadata": {
        "id": "aMo82WaHu3zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the label directories\n",
        "train_labels_path = 'data2/train/labels'\n",
        "val_labels_path = 'data2/valid/labels'\n",
        "test_labels_path = 'data2/test/labels'\n",
        "\n",
        "# Function to update labels in a given path\n",
        "def update_labels(path):\n",
        "    for label_file in os.listdir(path):\n",
        "        if label_file.endswith('.txt'):\n",
        "            file_path = os.path.join(path, label_file)\n",
        "            updated_lines = []\n",
        "            with open(file_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    label, *coords = line.split()\n",
        "                    label = int(label)\n",
        "                    if label == 0:\n",
        "                        label = 3\n",
        "                    if label == 1:\n",
        "                        label = 3\n",
        "                    if label == 2:\n",
        "                        label = 3\n",
        "                    updated_lines.append(f\"{label} {' '.join(coords)}\\n\")\n",
        "\n",
        "            with open(file_path, 'w') as file:\n",
        "                file.writelines(updated_lines)\n",
        "\n",
        "# Update labels in train, validation, and test sets\n",
        "for path in [train_labels_path]:\n",
        "    update_labels(path)\n",
        "\n",
        "print(\"Labels updated successfully.\")\n"
      ],
      "metadata": {
        "id": "hRAFXFj5u5zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset # 01**\n",
        "\n",
        "## Updating and standardising label files, deleting image files without labels.\n",
        "\n",
        "This step follows the same structure as with dataset 6, however this dataset did not need as much management of corrupt label files.\n",
        "\n",
        "The update labels function reads and updates the lbales in specified directories, remapping class 0 to class 2. It then updates the file with the new labels. Files without labels are also deleted."
      ],
      "metadata": {
        "id": "PDH3DR8-vLEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Paths to the label directories\n",
        "train_labels_path = 'data1/train/labels'\n",
        "val_labels_path = 'data1/valid/labels'\n",
        "test_labels_path = 'data1/test/labels'\n",
        "\n",
        "# Function to update labels in a given path\n",
        "def update_labels(path):\n",
        "    for label_file in os.listdir(path):\n",
        "        if label_file.endswith('.txt'):\n",
        "            file_path = os.path.join(path, label_file)\n",
        "            updated_lines = []\n",
        "            with open(file_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    label, *coords = line.split()\n",
        "                    label = int(label)\n",
        "                    if label == 0:\n",
        "                        label = 2\n",
        "                    updated_lines.append(f\"{label} {' '.join(coords)}\\n\")\n",
        "\n",
        "            with open(file_path, 'w') as file:\n",
        "                file.writelines(updated_lines)\n",
        "\n",
        "# Update labels in train, validation, and test sets\n",
        "for path in [train_labels_path]:\n",
        "    update_labels(path)\n",
        "\n",
        "print(\"Labels updated successfully.\")\n"
      ],
      "metadata": {
        "id": "qdKEGI8MvMtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Counting the File Quantity for Each Label**\n",
        "\n",
        "labels_path defines the path to the directory containting all of the label files for the combined datasets, and it initialises a dictionary to store those label counts.\n",
        "\n",
        "The count_labels(path) function then counts the occurences of each class label across all label files in the directory. These results are stored in the label_counts directory.\n",
        "\n",
        "The count_labels(labels_path) section then prints the number of instances for each class label, allowing me to check that the dataset is balanced."
      ],
      "metadata": {
        "id": "vbZ-4vfVvm2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the directory containing label files\n",
        "labels_path = 'complete_data/labels'\n",
        "\n",
        "# Dictionary to store the count of each class\n",
        "label_counts = {}\n",
        "# Function to count labels in a given path\n",
        "def count_labels(path):\n",
        "    for label_file in os.listdir(path):\n",
        "        if label_file.endswith('.txt'):\n",
        "            file_path = os.path.join(path, label_file)\n",
        "            with open(file_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    label, *coords = line.split()\n",
        "                    label = int(label)\n",
        "                    if label in label_counts:\n",
        "                        label_counts[label] += 1\n",
        "                    else:\n",
        "                        label_counts[label] = 1\n",
        "\n",
        "# Count labels in the specified directory\n",
        "count_labels(labels_path)\n",
        "\n",
        "# Print the counts of each class\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"Class {label}: {count} instances\")\n",
        "\n",
        "print(\"Completed counting labels.\")"
      ],
      "metadata": {
        "id": "v5d25hu5vvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check Label FIle Format Consistency**\n",
        "\n",
        "The check_label_files function checks each label file in the specified directories to ensure that each line contains exactly 5 values (class label and bbox coordinates). If any line has a format error, it is flagged up using the print function for later debugging."
      ],
      "metadata": {
        "id": "nb-In-bg5o_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check label files for format consistency\n",
        "def check_label_files(label_dirs):\n",
        "    for label_dir in label_dirs:\n",
        "        for label_file in os.listdir(label_dir):\n",
        "            if label_file.endswith('.txt'):\n",
        "                file_path = os.path.join(label_dir, label_file)\n",
        "                with open(file_path, 'r') as file:\n",
        "                    for line in file:\n",
        "                        values = line.split()\n",
        "                        if len(values) != 5:\n",
        "                            print(f\"Incorrect format in file {file_path}: {line.strip()}\")\n",
        "\n",
        "# List of label directories to check\n",
        "label_dirs = [\n",
        "    'data4/train/labels', 'data4/valid/labels', 'data4/test/labels',\n",
        "    'data3/train/labels', 'data3/valid/labels', 'data3/test/labels',\n",
        "    'data2/train/labels', 'data2/valid/labels', 'data2/test/labels',\n",
        "    'data1/train/labels'\n",
        "]\n",
        "\n",
        "# Check the label files for format issues\n",
        "check_label_files(label_dirs)\n"
      ],
      "metadata": {
        "id": "J5UF4cCzGkO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Augmentation Pipeline**\n",
        "\n",
        "First, the necessary libraries are imported for image augmentation and handling. E.g. albumentations is used for image augmentations, and ToTensorV2 converts the images to PyTorch tensors.\n",
        "\n",
        "The augmentations section then defines the augmentation pipeline with the following transformations, which will be outlined in more detail in the project report:\n",
        "\n",
        "\n",
        "*   Horizontal and vertical flips\n",
        "*   Random brightness and contrast adjustments\n",
        "*   Rotations, shifts, and scaling\n",
        "*   Converts the images to tensors for PyTorch compatibility\n",
        "\n",
        "This is followed by bbox_params, which ensures that the bboxes are correctly updated during transformations.\n",
        "\n"
      ],
      "metadata": {
        "id": "AjIyEC5Q6DBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmentation steps\n",
        "import os\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define augmentation pipeline\n",
        "augmentations = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Rotate(limit=30, p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.5),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "# Function to load image and corresponding labels\n",
        "def load_image_and_labels(image_path, labels_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width = image.shape[:2]\n",
        "    with open(labels_path, 'r') as file:\n",
        "        labels = []\n",
        "        for line in file:\n",
        "            class_label, x_center, y_center, bbox_width, bbox_height = map(float, line.split())\n",
        "            labels.append([class_label, x_center, y_center, bbox_width, bbox_height])\n",
        "    return image, labels, height, width\n",
        "\n",
        "# Function to save augmented image and labels\n",
        "def save_augmented_image_and_labels(image, labels, image_path, labels_path):\n",
        "    # Convert tensor to numpy array\n",
        "    image = image.permute(1, 2, 0).cpu().numpy()\n",
        "    image = (image * 255).astype(np.uint8)  # Convert to uint8\n",
        "\n",
        "    cv2.imwrite(image_path, image)\n",
        "    with open(labels_path, 'w') as file:\n",
        "        for label in labels:\n",
        "            file.write(' '.join(map(str, label)) + '\\n')\n",
        "\n",
        "# Function to augment dataset\n",
        "def augment_dataset(image_dir, label_dir, output_image_dir, output_label_dir, num_augmentations=1):\n",
        "    os.makedirs(output_image_dir, exist_ok=True)\n",
        "    os.makedirs(output_label_dir, exist_ok=True)\n",
        "\n",
        "    for image_file in os.listdir(image_dir):\n",
        "        if image_file.endswith(('.jpg', '.jpeg', '.png', '.webp')):\n",
        "            image_path = os.path.join(image_dir, image_file)\n",
        "            label_file = os.path.splitext(image_file)[0] + '.txt'\n",
        "            label_path = os.path.join(label_dir, label_file)\n",
        "\n",
        "            if os.path.exists(label_path):\n",
        "                image, labels, height, width = load_image_and_labels(image_path, label_path)\n",
        "\n",
        "                for i in range(num_augmentations):\n",
        "                    augmented = augmentations(image=image, bboxes=[label[1:] for label in labels], class_labels=[label[0] for label in labels])\n",
        "                    aug_image = augmented['image']\n",
        "                    aug_labels = [[augmented['class_labels'][j]] + list(augmented['bboxes'][j]) for j in range(len(augmented['bboxes']))]\n",
        "\n",
        "                    aug_image_path = os.path.join(output_image_dir, f\"{os.path.splitext(image_file)[0]}_aug_{i}.jpg\")\n",
        "                    aug_label_path = os.path.join(output_label_dir, f\"{os.path.splitext(image_file)[0]}_aug_{i}.txt\")\n",
        "\n",
        "                    save_augmented_image_and_labels(aug_image, aug_labels, aug_image_path, aug_label_path)\n",
        "\n",
        "    print(f\"Completed augmenting dataset in {image_dir}.\")\n",
        "\n",
        "# Apply augmentations to train, validation, and test sets\n",
        "datasets = [\n",
        "    ('data6/train/images', 'data6/train/labels', 'data6/train/aug_images', 'data6/train/aug_labels'),\n",
        "    ('data6/valid/images', 'data6/valid/labels', 'data6/valid/aug_images', 'data6/valid/aug_labels'),\n",
        "    ('data6/test/images', 'data6/test/labels', 'data6/test/aug_images', 'data6/test/aug_labels'),\n",
        "    ('data5/train/images', 'data5/train/labels', 'data5/train/aug_images', 'data5/train/aug_labels'),\n",
        "    ('data5/valid/images', 'data5/valid/labels', 'data5/valid/aug_images', 'data5/valid/aug_labels'),\n",
        "    ('data5/test/images', 'data5/test/labels', 'data5/test/aug_images', 'data5/test/aug_labels'),\n",
        "    ('data4/train/images', 'data4/train/labels', 'data4/train/aug_images', 'data4/train/aug_labels'),\n",
        "    ('data4/valid/images', 'data4/valid/labels', 'data4/valid/aug_images', 'data4/valid/aug_labels'),\n",
        "    ('data4/test/images', 'data4/test/labels', 'data4/test/aug_images', 'data4/test/aug_labels'),\n",
        "    ('data3/train/images', 'data3/train/labels', 'data3/train/aug_images', 'data3/train/aug_labels'),\n",
        "    ('data3/valid/images', 'data3/valid/labels', 'data3/valid/aug_images', 'data3/valid/aug_labels'),\n",
        "    ('data3/test/images', 'data3/test/labels', 'data3/test/aug_images', 'data3/test/aug_labels'),\n",
        "    ('data2/train/images', 'data2/train/labels', 'data2/train/aug_images', 'data2/train/aug_labels'),\n",
        "    ('data2/valid/images', 'data2/valid/labels', 'data2/valid/aug_images', 'data2/valid/aug_labels'),\n",
        "    ('data2/test/images', 'data2/test/labels', 'data2/test/aug_images', 'data2/test/aug_labels'),\n",
        "    ('data1/train/images', 'data1/train/labels', 'data1/train/aug_images', 'data1/train/aug_labels'),\n",
        "]\n",
        "\n",
        "for image_dir, label_dir, output_image_dir, output_label_dir in datasets:\n",
        "    augment_dataset(image_dir, label_dir, output_image_dir, output_label_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "MzQBiO-1v-vz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving the cleaned and augmentated datasets to the GDrive**\n",
        "\n",
        "GDrive is mounted to save the augmented datasets. The copy_to_drive function then copies the augmented images and labels from the local environment to the GDrive, ensuring that directories are created if they don't aready exist. It then iterates through the datasets and applies the copy_to_drive function, copying all of the data to GDrive directories.\n",
        "\n",
        "**NOTE: I then *manually* saved the unified augmented dataset to my local machine as a zip file, before splitting the dataset into train, val, and test sets once again in the main script. As such, GDrive is not used in the main project file, but it was a necessary step in this section due to the amount of data that was being handled.**\n",
        "\n"
      ],
      "metadata": {
        "id": "gnEVBX-J64KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save datasets\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define source and destination paths\n",
        "datasets = [\n",
        "    ('data6/train/aug_images', 'data6/train/aug_labels', '/content/drive/MyDrive/augmented_data6/train/aug_images', '/content/drive/MyDrive/augmented_data6/train/aug_labels'),\n",
        "    ('data6/valid/aug_images', 'data6/valid/aug_labels', '/content/drive/MyDrive/augmented_data6/valid/aug_images', '/content/drive/MyDrive/augmented_data6/valid/aug_labels'),\n",
        "    ('data6/test/aug_images', 'data6/test/aug_labels', '/content/drive/MyDrive/augmented_data6/test/aug_images', '/content/drive/MyDrive/augmented_data6/test/aug_labels'),\n",
        "    ('data5/train/aug_images', 'data5/train/aug_labels', '/content/drive/MyDrive/augmented_data5/train/aug_images', '/content/drive/MyDrive/augmented_data5/train/aug_labels'),\n",
        "    ('data5/valid/aug_images', 'data5/valid/aug_labels', '/content/drive/MyDrive/augmented_data5/valid/aug_images', '/content/drive/MyDrive/augmented_data5/valid/aug_labels'),\n",
        "    ('data5/test/aug_images', 'data5/test/aug_labels', '/content/drive/MyDrive/augmented_data5/test/aug_images', '/content/drive/MyDrive/augmented_data5/test/aug_labels'),\n",
        "    ('data4/train/aug_images', 'data4/train/aug_labels', '/content/drive/MyDrive/augmented_data4/train/aug_images', '/content/drive/MyDrive/augmented_data4/train/aug_labels'),\n",
        "    ('data4/valid/aug_images', 'data4/valid/aug_labels', '/content/drive/MyDrive/augmented_data4/valid/aug_images', '/content/drive/MyDrive/augmented_data4/valid/aug_labels'),\n",
        "    ('data4/test/aug_images', 'data4/test/aug_labels', '/content/drive/MyDrive/augmented_data4/test/aug_images', '/content/drive/MyDrive/augmented_data4/test/aug_labels'),\n",
        "    ('data3/train/aug_images', 'data3/train/aug_labels', '/content/drive/MyDrive/augmented_data3/train/aug_images', '/content/drive/MyDrive/augmented_data3/train/aug_labels'),\n",
        "    ('data3/valid/aug_images', 'data3/valid/aug_labels', '/content/drive/MyDrive/augmented_data3/valid/aug_images', '/content/drive/MyDrive/augmented_data3/valid/aug_labels'),\n",
        "    ('data3/test/aug_images', 'data3/test/aug_labels', '/content/drive/MyDrive/augmented_data3/test/aug_images', '/content/drive/MyDrive/augmented_data3/test/aug_labels'),\n",
        "    ('data2/train/aug_images', 'data2/train/aug_labels', '/content/drive/MyDrive/augmented_data2/train/aug_images', '/content/drive/MyDrive/augmented_data2/train/aug_labels'),\n",
        "    ('data2/valid/aug_images', 'data2/valid/aug_labels', '/content/drive/MyDrive/augmented_data2/valid/aug_images', '/content/drive/MyDrive/augmented_data2/valid/aug_labels'),\n",
        "    ('data2/test/aug_images', 'data2/test/aug_labels', '/content/drive/MyDrive/augmented_data2/test/aug_images', '/content/drive/MyDrive/augmented_data2/test/aug_labels'),\n",
        "    ('data1/train/aug_images', 'data1/train/aug_labels', '/content/drive/MyDrive/augmented_data1/train/aug_images', '/content/drive/MyDrive/augmented_data1/train/aug_labels'),\n",
        "]\n",
        "\n",
        "# Function to copy directories to Google Drive\n",
        "def copy_to_drive(src_image_dir, src_label_dir, dest_image_dir, dest_label_dir):\n",
        "    os.makedirs(dest_image_dir, exist_ok=True)\n",
        "    os.makedirs(dest_label_dir, exist_ok=True)\n",
        "\n",
        "    for file_name in os.listdir(src_image_dir):\n",
        "        full_file_name = os.path.join(src_image_dir, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, dest_image_dir)\n",
        "\n",
        "    for file_name in os.listdir(src_label_dir):\n",
        "        full_file_name = os.path.join(src_label_dir, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, dest_label_dir)\n",
        "\n",
        "# Copy augmented datasets to Google Drive\n",
        "for src_image_dir, src_label_dir, dest_image_dir, dest_label_dir in datasets:\n",
        "    copy_to_drive(src_image_dir, src_label_dir, dest_image_dir, dest_label_dir)\n",
        "\n",
        "print(\"Augmented datasets have been copied to Google Drive.\")\n"
      ],
      "metadata": {
        "id": "42UWfEYq7oDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}